# GLBA DistilBERT Model Configuration

model:
  name: "benjamin-glba"
  base_model: "distilbert-base-uncased"
  num_labels: 4
  dropout_rate: 0.3
  hidden_dim: 768
  freeze_bert: false

training:
  batch_size: 16
  learning_rate: 2e-5
  bert_learning_rate: 2e-5
  classifier_learning_rate: 1e-4
  num_epochs: 10
  max_length: 512
  weight_decay: 0.01
  adam_epsilon: 1e-8
  max_grad_norm: 1.0
  warmup_ratio: 0.1
  scheduler_type: "linear_warmup"  # Options: linear_warmup, reduce_on_plateau, cosine
  early_stopping_patience: 3
  use_class_weights: true

data:
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  max_samples_per_class: null
  use_text_cleaner: true
  preserve_legal_structure: false
  preserve_citations: false
  normalize_financial_terms: true

logging:
  use_wandb: false
  wandb_project: "glba-distilbert"
  log_level: "INFO"
  log_file: "logs/training.log"

paths:
  data_dir: "data"
  output_dir: "models"
  logs_dir: "logs"
  cache_dir: ".cache"

evaluation:
  metrics:
    - "accuracy"
    - "f1_weighted"
    - "precision_weighted"
    - "recall_weighted"
    - "matthews_corrcoef"
    - "violation_detection_rate"
  save_plots: true
  confusion_matrix: true
  classification_report: true

gpu:
  use_gpu: true
  device: "auto"  # auto, cuda, cpu
  mixed_precision: false